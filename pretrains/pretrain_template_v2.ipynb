{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e212769df2c79163",
   "metadata": {},
   "source": [
    "# Test with pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fa5d0748a13767e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:48:34.142931Z",
     "start_time": "2025-04-21T11:48:31.294135Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import cpuinfo\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a18de51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control randomness\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460da8d92d8804f",
   "metadata": {},
   "source": [
    "### File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7143543ffa0b1711",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:48:34.284882Z",
     "start_time": "2025-04-21T11:48:34.282781Z"
    }
   },
   "outputs": [],
   "source": [
    "train_path = \"../data/Train\"       # paths for your training and testing dataset\n",
    "#train_path = \"../data/aug_train\"    \n",
    "test_path = \"../data/Test\"\n",
    "input_parameter = \"\"                # paths for import and export custom model trainable parameters\n",
    "output_parameter = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61231f85",
   "metadata": {},
   "source": [
    "### Device of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08a1c831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU count: 16\n",
      "Using GPU: NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "device_name = \"\"\n",
    "\n",
    "print(f\"CPU count: {os.cpu_count()}\")\n",
    "num_workers = min(4, os.cpu_count() // 2)  # Dynamically set num_workers\n",
    "\n",
    "if device == torch.device(\"cuda\"):\n",
    "    device_name = torch.cuda.get_device_name(device)\n",
    "    print(f\"Using GPU: {device_name}\")\n",
    "else:\n",
    "    cpu_info = cpuinfo.get_cpu_info()\n",
    "    device_name = cpu_info['brand_raw']\n",
    "    print(f\"Using CPU: {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21140ca",
   "metadata": {},
   "source": [
    "### Pretrain Model of use from torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a34a8a8d04cb802",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:48:34.447142Z",
     "start_time": "2025-04-21T11:48:34.298297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model EfficientNet\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import efficientnet_b0\n",
    "model = efficientnet_b0(weights='DEFAULT')\n",
    "#model = efficientnet_v2_s(weights='DEFAULT')\n",
    "print(f\"Using model {type(model).__name__}\")\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 8 # adjust to your memory\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=3e-4,            # learning rate\n",
    "    weight_decay=3e-5,  # L2 regularization\n",
    "    betas=(0.9, 0.999), # Adam beta parameters\n",
    "    )\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.3,\n",
    "    patience=3,\n",
    "    min_lr=1e-6,\n",
    "    cooldown=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4332cc4f55b5e4bf",
   "metadata": {},
   "source": [
    "### Data Loader and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e0a404b216b26a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:48:34.517225Z",
     "start_time": "2025-04-21T11:48:34.493177Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: [114, 376, 95, 438, 357, 462, 77, 181, 139]\n",
      "Classes: ['actinic keratosis', 'basal cell carcinoma', 'dermatofibroma', 'melanoma', 'nevus', 'pigmented benign keratosis', 'seborrheic keratosis', 'squamous cell carcinoma', 'vascular lesion']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Transform for training and testing datasets\n",
    "img_size = 224 # adjust input image size for model\n",
    "transform_train = transforms.Compose([          # on training dataset\n",
    "    transforms.Resize(450),                         # Resize to 450x450\n",
    "    transforms.RandomRotation(45, expand=False),\n",
    "    transforms.CenterCrop(318),                     # Center crop to 450x450\n",
    "    transforms.Resize((224, 224)),                  # Resize to 224x224\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "transform_test = transforms.Compose([   # on test dataset\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop((224, 224)),                     # Center crop to 450x450\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(root=train_path, transform=transform_train)\n",
    "test_dataset = datasets.ImageFolder(root=test_path, transform=transform_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "train_data = [\n",
    "    (images.to(device), labels.to(device))\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Preloading Train Data to {device_name}\", leave=False)\n",
    "]\n",
    "val_data = [\n",
    "    (images.to(device), labels.to(device))\n",
    "    for images, labels in tqdm(test_loader, desc=f\"Preloading Val Data to {device_name}\", leave=False)\n",
    "]\n",
    "\n",
    "class_counts = [0] * len(train_dataset.classes)\n",
    "for _, label in train_dataset.samples:\n",
    "    class_counts[label] += 1\n",
    "\n",
    "print(f\"Class counts: {class_counts}\")\n",
    "print(f\"Classes: {train_dataset.classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1ea264",
   "metadata": {},
   "source": [
    "### Weighted Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0848f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([0.1597, 0.0484, 0.1917, 0.0416, 0.0510, 0.0394, 0.2365, 0.1006, 0.1310],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.arange(len(train_dataset.classes)),\n",
    "    y=[label for _, label in train_dataset.samples]\n",
    ")\n",
    "\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "normalized_class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(weight=normalized_class_weights, label_smoothing=0.1)\n",
    "\n",
    "print(f\"Class weights: {normalized_class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83c2871148fc4fc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:48:34.524281Z",
     "start_time": "2025-04-21T11:48:34.521216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier - Input features: 1280, Output classes: 9\n"
     ]
    }
   ],
   "source": [
    "# Edit the output layer of the model\n",
    "num_classes = len(train_dataset.classes)\n",
    "num_features = model.classifier[1].in_features\n",
    "print(f\"Classifier - Input features: {num_features}, Output classes: {num_classes}\")\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(num_features, num_classes),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1795a45e3c08a75",
   "metadata": {},
   "source": [
    "### Configure model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dda5db7349fc7d89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:48:34.565876Z",
     "start_time": "2025-04-21T11:48:34.563196Z"
    }
   },
   "outputs": [],
   "source": [
    "# IF NEEDED\n",
    "# Load custom weight and optimizer states\n",
    "# if os.path.exists(input_parameter):\n",
    "#     checkpoint = torch.load(\"test_weights.pth\", map_location=device)\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ded31e95efe3a765",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:48:34.588042Z",
     "start_time": "2025-04-21T11:48:34.585548Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Selective layer freezing\n",
    "# # change base on your model\n",
    "# # \"Early layers are often already well-optimized\" by GPT-4o\n",
    "# for _, param in model.named_parameters():\n",
    "#     param.requires_grad = True          # Unfreeze all layers first\n",
    "# for name, param in model.named_parameters():\n",
    "#     if \"classifier\" not in name:\n",
    "#         param.requires_grad = False     # Freeze everything except the last classifier layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3247b01c5c37b276",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T11:48:34.687917Z",
     "start_time": "2025-04-21T11:48:34.598546Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Move model to device\n",
    "model.to(device)\n",
    "print(f\"Model is on {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69e6c43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================================\n",
      "Layer (type:depth-idx)                                  Output Shape              Param #\n",
      "=========================================================================================================\n",
      "EfficientNet                                            [8, 9]                    --\n",
      "├─Sequential: 1-1                                       [8, 1280, 7, 7]           --\n",
      "│    └─Conv2dNormActivation: 2-1                        [8, 32, 112, 112]         --\n",
      "│    │    └─Conv2d: 3-1                                 [8, 32, 112, 112]         864\n",
      "│    │    └─BatchNorm2d: 3-2                            [8, 32, 112, 112]         64\n",
      "│    │    └─SiLU: 3-3                                   [8, 32, 112, 112]         --\n",
      "│    └─Sequential: 2-2                                  [8, 16, 112, 112]         --\n",
      "│    │    └─MBConv: 3-4                                 [8, 16, 112, 112]         1,448\n",
      "│    └─Sequential: 2-3                                  [8, 24, 56, 56]           --\n",
      "│    │    └─MBConv: 3-5                                 [8, 24, 56, 56]           6,004\n",
      "│    │    └─MBConv: 3-6                                 [8, 24, 56, 56]           10,710\n",
      "│    └─Sequential: 2-4                                  [8, 40, 28, 28]           --\n",
      "│    │    └─MBConv: 3-7                                 [8, 40, 28, 28]           15,350\n",
      "│    │    └─MBConv: 3-8                                 [8, 40, 28, 28]           31,290\n",
      "│    └─Sequential: 2-5                                  [8, 80, 14, 14]           --\n",
      "│    │    └─MBConv: 3-9                                 [8, 80, 14, 14]           37,130\n",
      "│    │    └─MBConv: 3-10                                [8, 80, 14, 14]           102,900\n",
      "│    │    └─MBConv: 3-11                                [8, 80, 14, 14]           102,900\n",
      "│    └─Sequential: 2-6                                  [8, 112, 14, 14]          --\n",
      "│    │    └─MBConv: 3-12                                [8, 112, 14, 14]          126,004\n",
      "│    │    └─MBConv: 3-13                                [8, 112, 14, 14]          208,572\n",
      "│    │    └─MBConv: 3-14                                [8, 112, 14, 14]          208,572\n",
      "│    └─Sequential: 2-7                                  [8, 192, 7, 7]            --\n",
      "│    │    └─MBConv: 3-15                                [8, 192, 7, 7]            262,492\n",
      "│    │    └─MBConv: 3-16                                [8, 192, 7, 7]            587,952\n",
      "│    │    └─MBConv: 3-17                                [8, 192, 7, 7]            587,952\n",
      "│    │    └─MBConv: 3-18                                [8, 192, 7, 7]            587,952\n",
      "│    └─Sequential: 2-8                                  [8, 320, 7, 7]            --\n",
      "│    │    └─MBConv: 3-19                                [8, 320, 7, 7]            717,232\n",
      "│    └─Conv2dNormActivation: 2-9                        [8, 1280, 7, 7]           --\n",
      "│    │    └─Conv2d: 3-20                                [8, 1280, 7, 7]           409,600\n",
      "│    │    └─BatchNorm2d: 3-21                           [8, 1280, 7, 7]           2,560\n",
      "│    │    └─SiLU: 3-22                                  [8, 1280, 7, 7]           --\n",
      "├─AdaptiveAvgPool2d: 1-2                                [8, 1280, 1, 1]           --\n",
      "├─Sequential: 1-3                                       [8, 9]                    --\n",
      "│    └─Dropout: 2-10                                    [8, 1280]                 --\n",
      "│    └─Linear: 2-11                                     [8, 9]                    11,529\n",
      "=========================================================================================================\n",
      "Total params: 4,019,077\n",
      "Trainable params: 4,019,077\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 3.08\n",
      "=========================================================================================================\n",
      "Input size (MB): 4.82\n",
      "Forward/backward pass size (MB): 863.02\n",
      "Params size (MB): 16.08\n",
      "Estimated Total Size (MB): 883.92\n",
      "=========================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Print model architecture\n",
    "print(summary(model, (batch_size, 3, 224, 224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22466871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DEBUG\n",
    "# print(f\"Model device: {next(model.parameters()).device}\")\n",
    "# for images, labels in train_data:\n",
    "#     print(f\"Input device: {images.device}\")\n",
    "#     break\n",
    "# print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c027cba37344acf9",
   "metadata": {},
   "source": [
    "### Training Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3520fd42aeea885",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T12:11:52.251397Z",
     "start_time": "2025-04-21T11:48:34.690910Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Learning Rate: 0.0003000 | Train Loss: 1.7922, Train Acc: 45.73% | Val Loss: 1.8648, Val Acc: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 - Learning Rate: 0.0003000 | Train Loss: 1.3354, Train Acc: 67.71% | Val Loss: 1.9010, Val Acc: 48.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 - Learning Rate: 0.0003000 | Train Loss: 1.0569, Train Acc: 82.76% | Val Loss: 1.9729, Val Acc: 47.46%\n",
      "Augmenting dataset after epoch 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 - Learning Rate: 0.0003000 | Train Loss: 1.5141, Train Acc: 59.36% | Val Loss: 1.7275, Val Acc: 55.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 - Learning Rate: 0.0003000 | Train Loss: 1.1096, Train Acc: 78.16% | Val Loss: 1.7141, Val Acc: 59.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 - Learning Rate: 0.0003000 | Train Loss: 0.9182, Train Acc: 90.49% | Val Loss: 1.8017, Val Acc: 59.32%\n",
      "Augmenting dataset after epoch 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 - Learning Rate: 0.0003000 | Train Loss: 1.4023, Train Acc: 65.07% | Val Loss: 1.8367, Val Acc: 55.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 - Learning Rate: 0.0003000 | Train Loss: 1.0521, Train Acc: 82.18% | Val Loss: 1.8419, Val Acc: 58.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 - Learning Rate: 0.0003000 | Train Loss: 0.8847, Train Acc: 93.21% | Val Loss: 1.7560, Val Acc: 61.02%\n",
      "Augmenting dataset after epoch 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 - Learning Rate: 0.0000900 | Train Loss: 1.2480, Train Acc: 72.18% | Val Loss: 1.6586, Val Acc: 59.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 - Learning Rate: 0.0000900 | Train Loss: 0.9581, Train Acc: 88.79% | Val Loss: 1.6770, Val Acc: 61.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 - Learning Rate: 0.0000900 | Train Loss: 0.8621, Train Acc: 95.00% | Val Loss: 1.6547, Val Acc: 61.86%\n",
      "Augmenting dataset after epoch 12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 - Learning Rate: 0.0000900 | Train Loss: 1.2033, Train Acc: 76.82% | Val Loss: 1.6332, Val Acc: 63.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 - Learning Rate: 0.0000900 | Train Loss: 0.9260, Train Acc: 90.00% | Val Loss: 1.6837, Val Acc: 65.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 - Learning Rate: 0.0000900 | Train Loss: 0.8393, Train Acc: 95.40% | Val Loss: 1.7078, Val Acc: 65.25%\n",
      "Augmenting dataset after epoch 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 - Learning Rate: 0.0000900 | Train Loss: 1.1571, Train Acc: 78.47% | Val Loss: 1.7372, Val Acc: 62.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 - Learning Rate: 0.0000900 | Train Loss: 0.9105, Train Acc: 90.49% | Val Loss: 1.7143, Val Acc: 62.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 - Learning Rate: 0.0000270 | Train Loss: 0.8206, Train Acc: 96.43% | Val Loss: 1.7287, Val Acc: 64.41%\n",
      "Augmenting dataset after epoch 18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 - Learning Rate: 0.0000270 | Train Loss: 1.0946, Train Acc: 81.87% | Val Loss: 1.6905, Val Acc: 66.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 - Learning Rate: 0.0000270 | Train Loss: 0.9652, Train Acc: 87.58% | Val Loss: 1.7018, Val Acc: 65.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 - Learning Rate: 0.0000270 | Train Loss: 0.9017, Train Acc: 91.38% | Val Loss: 1.7230, Val Acc: 64.41%\n",
      "Augmenting dataset after epoch 21...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 - Learning Rate: 0.0000270 | Train Loss: 1.0822, Train Acc: 82.36% | Val Loss: 1.6147, Val Acc: 68.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 - Learning Rate: 0.0000270 | Train Loss: 0.9653, Train Acc: 87.94% | Val Loss: 1.6162, Val Acc: 68.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 - Learning Rate: 0.0000270 | Train Loss: 0.9042, Train Acc: 91.25% | Val Loss: 1.6363, Val Acc: 71.19%\n",
      "Augmenting dataset after epoch 24...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 - Learning Rate: 0.0000270 | Train Loss: 1.0792, Train Acc: 82.27% | Val Loss: 1.6642, Val Acc: 67.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 - Learning Rate: 0.0000270 | Train Loss: 0.9643, Train Acc: 87.85% | Val Loss: 1.6830, Val Acc: 65.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 - Learning Rate: 0.0000081 | Train Loss: 0.9027, Train Acc: 91.34% | Val Loss: 1.6960, Val Acc: 63.56%\n",
      "Augmenting dataset after epoch 27...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 - Learning Rate: 0.0000081 | Train Loss: 1.0279, Train Acc: 84.28% | Val Loss: 1.7767, Val Acc: 63.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 - Learning Rate: 0.0000081 | Train Loss: 0.9901, Train Acc: 85.93% | Val Loss: 1.7575, Val Acc: 64.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 - Learning Rate: 0.0000081 | Train Loss: 0.9633, Train Acc: 86.91% | Val Loss: 1.7673, Val Acc: 63.56%\n",
      "CPU times: total: 5min 38s\n",
      "Wall time: 8min 48s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "previous_loss = float('inf')\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    train_bar = tqdm(train_data, desc=f\"Epoch {epoch + 1}/{epochs} [Train]\", leave=False)\n",
    "    for images, labels in train_bar:\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate statistics\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        # Update progress bar\n",
    "        train_bar.set_postfix({\n",
    "            'loss': f\"{running_loss / total_train:.4f}\",\n",
    "            'acc': f\"{100. * correct_train / total_train:.2f}%\"\n",
    "        })\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    val_bar = tqdm(val_data, desc=f\"Epoch {epoch + 1}/{epochs} [Val]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_bar:\n",
    "            outputs = model(images)\n",
    "            loss = loss_function(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "            # Update progress bar\n",
    "            val_bar.set_postfix({\n",
    "                'loss': f\"{val_loss / total_val:.4f}\",\n",
    "                'acc': f\"{100. * correct_val / total_val:.2f}%\"\n",
    "            })\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss / total_val)\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} - \"\n",
    "          f\"Learning Rate: {current_lr:.7f} | \"\n",
    "          f\"Train Loss: {running_loss / total_train:.4f}, Train Acc: {100. * correct_train / total_train:.2f}% | \"\n",
    "          f\"Val Loss: {val_loss / total_val:.4f}, Val Acc: {100. * correct_val / total_val:.2f}%\")\n",
    "\n",
    "    # Augment the dataset for every 3 epochs\n",
    "    if (epoch + 1) % 3 == 0 and epoch != (epochs - 1):\n",
    "        print(f\"Augmenting dataset after epoch {epoch + 1}...\")\n",
    "        del train_data\n",
    "        torch.cuda.empty_cache()\n",
    "        train_dataset = datasets.ImageFolder(root=train_path, transform=transform_train)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "        train_data = [\n",
    "            (images.to(device), labels.to(device))\n",
    "            for images, labels in tqdm(train_loader, desc=f\"Preloading Train Data to {device_name}\", leave=False)\n",
    "        ]\n",
    "    \n",
    "    # # Augment the dataset if validation loss is not improving\n",
    "    # if val_loss > previous_loss:\n",
    "    #     times = 0\n",
    "    #     print(f\"Val loss did not improve from {previous_loss / total_val:.4f} at epoch {previous_epoch}. Augmenting dataset...\")\n",
    "    #     del train_data\n",
    "    #     torch.cuda.empty_cache()\n",
    "    #     train_dataset = datasets.ImageFolder(root=train_path, transform=transform_train)\n",
    "    #     train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    #     train_data = [\n",
    "    #         (images.to(device), labels.to(device))\n",
    "    #         for images, labels in tqdm(train_loader, desc=f\"Preloading Train Data to {device_name}\", leave=False)\n",
    "    #     ]\n",
    "    # if val_loss < previous_loss:\n",
    "    #     previous_loss = val_loss\n",
    "    #     previous_epoch = epoch\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f05590085d19fe",
   "metadata": {},
   "source": [
    "### Training log and data export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cbef08ee604230",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T12:11:52.411762Z",
     "start_time": "2025-04-21T12:11:52.409200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Code here TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e080da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration 5 template 2\n",
    "# Updates:\n",
    "# 1. Manual seed to reduce randomness\n",
    "# 2. Use ReduceLROnPlateau as lr scheduler\n",
    "# 3. Improved transformation to account black paddings due to rotation\n",
    "# 4. Augmentation now only happen for every 3 epoch instead checking for validation loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSC671-S25-TeamProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
